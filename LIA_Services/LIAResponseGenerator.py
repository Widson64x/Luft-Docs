# /ai_services/response_generator.py
import re
from .Configs.LLMConfig import get_client

SYSTEM_PROMPT = """Voc√™ √© a 'Lia', a assistente de conhecimento gente boa da Luft. Sua miss√£o √© ajudar seus colegas de equipe a encontrar informa√ß√µes de forma clara, amig√°vel e descomplicada. Pense e responda como se fosse um colega de trabalho brasileiro: prestativo, um pouco informal e que vai direto ao ponto sem ser rob√≥tico. Use um toque de "malemol√™ncia" e bom humor. Voc√™ √© a funcion√°ria Luft que tem o dever de responder tudo que se sabe da Luft com base no contexto.

**Suas regras de ouro para responder:**

1.  **Personalidade:**
    * **Sauda√ß√£o:** Comece sempre com um "Opa, vamos l√°!", "Beleza! Encontrei isso aqui pra voc√™:" ou algo nesse estilo descontra√≠do.
    * **Tom:** Mantenha um tom conversacional e prestativo. Use emojis para dar um toque de personalidade quando fizer sentido (üòâ, üëç).
    * **Despedida:** Termine com algo amig√°vel como "Qualquer outra d√∫vida, √© s√≥ chamar!" ou "Espero que ajude!".

2.  **Estrutura da Resposta (Use Markdown):**
    * **S√≠ntese:** Comece com um par√°grafo curto resumindo a resposta direta para a pergunta.
    * **T√≠tulos e Destaques:** Use t√≠tulos com `##` para separar se√ß√µes e negrito (`**palavra**`) para destacar termos importantes.
    * **Listas:** Se for um processo com passos, use listas com marcadores (`*`).
    * **REGRA CR√çTICA PARA IMAGENS:** Se o contexto contiver um caminho para uma imagem (ex: `/data/img/tela/foto.png`), √â SUA OBRIGA√á√ÉO **INCLUIR O CAMINHO EXATO DO ARQUIVO COMO TEXTO SIMPLES** na sua resposta. NUNCA formate o caminho da imagem como Markdown (N√ÉO use '![alt](caminho)'). Apenas escreva o caminho do arquivo diretamente no texto. O sistema se encarregar√° da formata√ß√£o final.

3.  **Foco e Honestidade:**
    * IMPORTANTE: NUNCA traga informa√ß√µes que n√£o estejam explicitamente presentes no contexto acima.
    * Se n√£o achar a resposta, apenas diga que n√£o foi poss√≠vel encontrar nos documentos.
    * N√£o chute, n√£o deduza, n√£o invente. Seja 100% fiel ao que est√° no contexto.

Lembre-se: seja a colega de trabalho que todo mundo gostaria de ter para tirar uma d√∫vida!"""
#PS: √Äs vezes eu me pergunto... quem veio primeiro, o ovo ou o deploy em produ√ß√£o? ü§î

def rerank_and_filter_context(question, documents, metadatas):
    """
    Usa um modelo de linguagem poderoso (reranker) para reclassificar os documentos,
    retornando o contexto mais relevante e suas fontes.
    """
    print(f"Iniciando re-ranking de {len(documents)} documentos candidatos...")
    if not documents:
        return "", []

    reranker_prompt = f"""
Sua tarefa √© atuar como um rigoroso assistente de pesquisa. Analise a PERGUNTA DO USU√ÅRIO e a lista de DOCUMENTOS numerados abaixo.
Para cada documento, avalie o qu√£o diretamente ele responde √† pergunta do usu√°rio. Sua avalia√ß√£o deve ser cr√≠tica.

Responda com uma lista em Python dos n√∫meros dos documentos que s√£o MAIS RELEVANTES e √öTEIS, em ordem de import√¢ncia.
O formato da resposta deve ser apenas a lista, como: `[3, 1, 5]`

N√£o explique sua decis√£o. Se nenhum documento for relevante, retorne uma lista vazia `[]`.

---
PERGUNTA DO USU√ÅRIO: "{question}"
---
DOCUMENTOS:
"""
    for i, doc in enumerate(documents):
        source = metadatas[i].get('source', 'desconhecida')
        reranker_prompt += f"\n{i+1}. [Fonte: {source}]:\n\"\"\"\n{doc}\n\"\"\"\n"

    try:
        # Usamos um modelo poderoso para esta tarefa de racioc√≠nio.
        print("Usando Groq (Llama 3.3 70b) como reranker...")
        groq_client = get_client('groq_client')
        if not groq_client:
            raise ConnectionError("Cliente Groq n√£o dispon√≠vel para re-ranking.")

        chat_completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": reranker_prompt}],
            model="llama-3.3-70b-versatile", # <-- NOVA CORRE√á√ÉO 1
            temperature=0.0, # Baixa temperatura para uma resposta focada e determin√≠stica
        )
        response_text = chat_completion.choices[0].message.content
        
        # Extrai a lista de n√∫meros da resposta do modelo
        ranked_indices_str = re.findall(r'\d+', response_text)
        ranked_indices = [int(i) - 1 for i in ranked_indices_str] # Converte para √≠ndices base-0

        print(f"Ordem de relev√¢ncia definida pelo reranker: {[i+1 for i in ranked_indices]}")

        # Pega os 4 melhores documentos, conforme a ordem do reranker
        top_k = 4
        final_docs = []
        final_metas = []
        for i in ranked_indices:
            if i < len(documents):
                final_docs.append(documents[i])
                final_metas.append(metadatas[i])
        
        final_docs = final_docs[:top_k]
        final_metas = final_metas[:top_k]

        if not final_docs:
            print("Reranker concluiu que nenhum documento √© relevante.")
            return "", []

        final_context = "\n---\n".join(final_docs)
        final_sources = [meta['source'] for meta in final_metas]
        
        print(f"Contexto final constru√≠do a partir de {len(final_docs)} documentos re-rankeados.")
        return final_context, sorted(list(set(final_sources)))

    except Exception as e:
        print(f"ERRO no re-ranking: {e}. Usando os documentos originais como fallback.")
        # Fallback: se o re-ranking falhar, usa os 3 primeiros documentos da busca original
        top_k = 3
        fallback_context = "\n---\n".join(documents[:top_k])
        fallback_sources = [meta['source'] for meta in metadatas[:top_k]]
        return fallback_context, sorted(list(set(fallback_sources)))

def generate_llm_answer(model_name, context, question):
    """Gera uma resposta usando o modelo de linguagem selecionado e o contexto j√° refinado."""
    human_prompt = f"**Contexto da Documenta√ß√£o:**\n{context}\n\n**Pergunta do Usu√°rio:** \"{question}\""
    answer = ""
    
    print(f"Gerando resposta com {model_name}...")

    if model_name == 'groq-70b':
        print("Gerando resposta com Groq (Llama 3.3 70b - Poderoso)...")
        groq_client = get_client('groq_client')
        chat_completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": human_prompt}], model="llama-3.3-70b-versatile") # <-- NOVA CORRE√á√ÉO 2
        answer = chat_completion.choices[0].message.content

    elif model_name == 'kimi':
        print("Gerando resposta com Moonshot Kimi (via Groq)...")
        groq_client = get_client('groq_client')
        chat_completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": human_prompt}], model="moonshotai/kimi-k2-instruct")
        answer = chat_completion.choices[0].message.content

    elif model_name == 'groq-8b':
        print("Gerando resposta com Groq (Llama 3 8b - R√°pido)...")
        groq_client = get_client('groq_client')
        chat_completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": human_prompt}], model="llama3-8b-8192")
        answer = chat_completion.choices[0].message.content

    elif model_name == 'openai' and get_client('openai_client'):
        print("Gerando resposta com OpenAI (GPT-4o)...")
        openai_client = get_client('openai_client')
        chat_completion = openai_client.chat.completions.create(messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": human_prompt}], model="gpt-4o")
        answer = chat_completion.choices[0].message.content

    elif model_name == 'gemini':
        print("Gerando resposta com Gemini (1.5 Flash)...")
        gemini_model = get_client('gemini_model')
        gemini_prompt = f"{SYSTEM_PROMPT}\n---\n{human_prompt}"
        response = gemini_model.generate_content(gemini_prompt)
        answer = response.text

    # --- IN√çCIO DA NOVA ADI√á√ÉO (OPENROUTER) ---
    # O 'model_name' aqui √© o que voc√™ vai colocar no <option> do seu HTML
    elif model_name == 'openrouter-gemini' and get_client('openrouter_client'):
        print("Gerando resposta com OpenRouter (google/gemini-2.5-flash)...")
        openrouter_client = get_client('openrouter_client')
        
        # A chamada √© id√™ntica √† do OpenAI/Groq
        chat_completion = openrouter_client.chat.completions.create(
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": human_prompt}
            ],
            model="google/gemini-2.5-flash" # Este √© o nome do modelo que o OpenRouter espera
        )
        answer = chat_completion.choices[0].message.content
    # --- FIM DA NOVA ADI√á√ÉO ---

    else:
        raise ValueError(f"Modelo '{model_name}' inv√°lido ou n√£o configurado.")
    
    return _force_image_formatting(answer)

def _force_image_formatting(text):
    """Garante que os caminhos de imagem sejam formatados corretamente como Markdown."""
    
    # Este padr√£o agora encontra o caminho da imagem, mesmo que n√£o tenha o prefixo /luft-docs
    # Ele captura o caminho a partir de "/data/img/..."
    image_pattern = r'(/data/img/[^\s\)<]+\.(png|jpg|jpeg|gif))'
    
    # Montamos a URL completa na substitui√ß√£o, adicionando o prefixo do servidor e o /luft-docs
    # O `\1` representa o caminho da imagem que foi encontrado (ex: /data/img/recebimento-intec/img2.png)
    replacement_format = r'\n\n![Imagem do Documento](http://127.0.0.1:9100/luft-docs\1)\n\n' or r'\n\n![Imagem do Documento](http://b2bi-apps.luftfarma.com.br/luft-docs\1)\n\n'
    
    # A fun√ß√£o `re.sub` vai encontrar todos os caminhos e aplicar a formata√ß√£o
    final_text = re.sub(image_pattern, replacement_format, text)
    
    return final_text